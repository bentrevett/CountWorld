{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import utils\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, word2idx, idx2word = utils.load_data('data', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "CLIP = 40\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "EMB_DIM = 50\n",
    "SENT_LEN = train_data[0][0].shape[1]\n",
    "STORY_LEN = train_data[0][0].shape[0]\n",
    "QUERY_LEN = train_data[0][1].shape[1]\n",
    "POS_ENC = True\n",
    "TEMP_ENC = True\n",
    "LS = True\n",
    "N_HOPS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'to': 1, 'jane': 2, 'how': 3, '12': 4, 'were': 5, 'mushrooms': 6, '14': 7, 'adam': 8, 'mountains': 9, 'station': 10, 'sticks': 11, 'different': 12, 'eric': 13, '8': 14, 'oliver': 15, '3': 16, 'emma': 17, 'bridge': 18, '9': 19, 'entities': 20, '2': 21, 'claire': 22, 'many': 23, 'is': 24, 'picked': 25, 'park': 26, '5': 27, 'the': 28, 'feathers': 29, 'ruben': 30, 'flowers': 31, 'forest': 32, 'drop': 33, 'rocks': 34, '11': 35, 'shells': 36, 'beach': 37, 'from': 38, 'berries': 39, 'eve': 40, 'pick': 41, '16': 42, 'river': 43, '7': 44, 'school': 45, 'sophie': 46, 'dropped': 47, 'insects': 48, '4': 49, 'liam': 50, 'was': 51, 'town': 52, 'times': 53, 'carrying': 54, 'stadium': 55, 'leaves': 56, 'visited': 57, 'visit': 58, '6': 59, 'went': 60, 'in': 61, 'at': 62, 'eggs': 63, 'up': 64, 'objects': 65, 'total': 66, '10': 67, '13': 68, 'did': 69, '?': 70, '1': 71}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 15,  60,   1,  28,  45],\n",
      "        [ 13,  60,   1,  28,  45],\n",
      "        [ 15,  60,   1,  28,  37],\n",
      "        [ 15,  25,  64,  71,  34],\n",
      "        [ 13,  25,  64,  16,   6],\n",
      "        [ 13,  60,   1,  28,  37],\n",
      "        [ 13,  47,  16,   6,   0],\n",
      "        [ 15,  60,   1,  28,  45],\n",
      "        [ 13,  25,  64,  71,   6],\n",
      "        [ 13,  60,   1,  28,  45],\n",
      "        [ 15,  60,   1,  28,  37],\n",
      "        [ 15,  25,  64,  21,   6],\n",
      "        [ 15,  47,  71,   6,   0],\n",
      "        [ 15,  60,   1,  28,  45],\n",
      "        [ 13,  25,  64,  16,   6],\n",
      "        [ 15,  25,  64,  21,  34],\n",
      "        [ 15,  25,  64,  21,   6],\n",
      "        [ 13,  25,  64,  21,  34],\n",
      "        [ 15,  60,   1,  28,  37],\n",
      "        [ 15,  25,  64,  16,   6]], device='cuda:0'), tensor([[  3,  23,  53,   5,  34,  25,  64,  38,  28,  37,  70]], device='cuda:0'), tensor([[ 1]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5]) torch.Size([1, 11]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][0].shape, train_data[0][1].shape, train_data[0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_iterator = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_iterator = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, sent_len, story_len, pos_enc, temp_enc, n_hops, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.sent_len = sent_len\n",
    "        self.story_len = story_len\n",
    "        self.pos_enc = pos_enc\n",
    "        self.temp_enc = temp_enc\n",
    "        self.n_hops = n_hops\n",
    "        \n",
    "        #input, query and output embeddings\n",
    "        #for nn.ModuleList, see: \n",
    "        #  https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=0) for _ in range(self.n_hops+1)])\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.normal_(0, 0.1)\n",
    "            e.weight.data[0].fill_(0)\n",
    "                \n",
    "        #calculate position encoding\n",
    "        if self.pos_enc:\n",
    "            J = self.sent_len\n",
    "            d = self.emb_dim\n",
    "            self.l = torch.zeros(J, d).to(device)\n",
    "            for j in range(1, J+1):\n",
    "                for k in range(1, d+1):\n",
    "                    self.l[j-1][k-1] = (1 - j/J) - (k/d) * (1 - 2*j/J)\n",
    "            self.l = self.l.unsqueeze(0).repeat(self.story_len, 1, 1) # l = [story len, sent len, emb dim]\n",
    "        \n",
    "        #initialize temporal encoding parameters\n",
    "        if self.temp_enc:\n",
    "            self.T_A = nn.Parameter(torch.randn(self.story_len, self.emb_dim).normal_(0, 0.1))\n",
    "            self.T_C = nn.Parameter(torch.randn(self.story_len, self.emb_dim).normal_(0, 0.1))\n",
    "        \n",
    "    def forward(self, S, Q, linear):\n",
    "        \n",
    "        # S = [bsz, story len, sent len]\n",
    "        # Q = [bsz, q len]\n",
    "        \n",
    "        #make sure input is the correct size\n",
    "        assert S.shape[1] == self.story_len and S.shape[2] == self.sent_len\n",
    "        \n",
    "        #embed the query \n",
    "        # B is the first embedding\n",
    "        U = self.embeddings[0](Q) # U = [bsz, q len, emb dim]\n",
    "        U = torch.sum(U, 1) # U = [bsz, emb dim]\n",
    "        \n",
    "        for k in range(self.n_hops):\n",
    "        \n",
    "            #embed the story\n",
    "            # A is embedding k, A^k, where k is the current hop number\n",
    "            M = self.embeddings[k](S) # M = [bsz, story len, sent_len, emb dim]\n",
    "\n",
    "            #apply position encoding\n",
    "            if self.pos_enc:\n",
    "                l = self.l.unsqueeze(0).repeat(M.shape[0], 1, 1, 1) # l = [bsz, story len, sent len, emb dim]\n",
    "                M *= l\n",
    "\n",
    "            M = torch.sum(M, 2) # M = [bsz, story len, emb dim]\n",
    "\n",
    "            #apply temporal encoding\n",
    "            if self.temp_enc:\n",
    "                T_A = self.T_A.unsqueeze(0).repeat(M.shape[0], 1, 1)\n",
    "                M += T_A\n",
    "\n",
    "            #calculate attention\n",
    "            P = torch.bmm(M, U.unsqueeze(2)).squeeze(2) # P = [bsz, story len]\n",
    "            if not linear:\n",
    "                P = F.softmax(P, dim=1) # P = [bsz, story len]\n",
    "\n",
    "            #output embedding of story\n",
    "            # C is embedding k+1, A^(k+1), where k is the current hop number\n",
    "            C = self.embeddings[k+1](S) # C = [bsz, story len, sent_len, emb dim]\n",
    "            \n",
    "            #apply position encoding\n",
    "            if self.pos_enc:\n",
    "                l = self.l.unsqueeze(0).repeat(C.shape[0], 1, 1, 1) # l = [bsz, story len, sent len, emb dim]\n",
    "                C *= l\n",
    "            \n",
    "            C = torch.sum(C, 2) # C = [bsz, story len, emb dim]\n",
    "\n",
    "            #apply temporal encoding\n",
    "            if self.temp_enc:\n",
    "                T_C = self.T_C.unsqueeze(0).repeat(C.shape[0], 1, 1)\n",
    "                C += T_C\n",
    "\n",
    "            #apply attention to output embedding\n",
    "            O = torch.bmm(P.unsqueeze(1), C).squeeze(1) # O = [bsz, emb dim]\n",
    "            \n",
    "            #the next embedded query is the sum of the previous embedded query and the output\n",
    "            U = U + O\n",
    "                \n",
    "        #get and reshape W\n",
    "        # W is embedding K, A^K, where K is the total number of hops (can also get with self.embeddings[-1])\n",
    "        W = self.embeddings[self.n_hops].weight.unsqueeze(0)\n",
    "        W = W.repeat(U.shape[0], 1, 1) #W = [bsz, vocab size, emb dim]\n",
    "               \n",
    "        #get probability distribution over vocab\n",
    "        A = torch.bmm(W, U.unsqueeze(2)).squeeze(2) # A = [bsz, vocab size]\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemoryNetwork(VOCAB_SIZE, EMB_DIM, SENT_LEN, STORY_LEN, POS_ENC, TEMP_ENC, N_HOPS, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, linear):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for s, q, a in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(s, q.squeeze(1), linear)\n",
    "\n",
    "        loss = criterion(predictions, a.squeeze(1).squeeze(1))\n",
    "        \n",
    "        top_pred = predictions.max(1, keepdim=True)[1] \n",
    "        acc = (top_pred == a.squeeze(1)).sum().float()/predictions.shape[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, optimizer, criterion, linear):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for s, q, a in iterator:\n",
    "\n",
    "            predictions = model(s, q.squeeze(1), linear)\n",
    "\n",
    "            loss = criterion(predictions, a.squeeze(1).squeeze(1))\n",
    "            \n",
    "            top_pred = predictions.max(1, keepdim=True)[1]\n",
    "            acc = (top_pred == a.squeeze(1)).sum().float()/predictions.shape[0]\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.771, Train Acc: 035.06%, Val. Loss: 1.454, Val. Acc: 040.10%, LR: 1.000000e-03, Linear: True\n",
      "Epoch: 002, Train Loss: 1.405, Train Acc: 042.01%, Val. Loss: 1.409, Val. Acc: 042.15%, LR: 1.000000e-03, Linear: True\n",
      "Epoch: 003, Train Loss: 1.367, Train Acc: 043.39%, Val. Loss: 1.361, Val. Acc: 043.52%, LR: 1.000000e-03, Linear: True\n",
      "Epoch: 004, Train Loss: 1.352, Train Acc: 044.06%, Val. Loss: 1.367, Val. Acc: 042.67%, LR: 1.000000e-03, Linear: True\n",
      "Epoch: 005, Train Loss: 1.548, Train Acc: 037.54%, Val. Loss: 1.409, Val. Acc: 040.23%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 006, Train Loss: 1.294, Train Acc: 046.78%, Val. Loss: 1.210, Val. Acc: 050.36%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 007, Train Loss: 1.154, Train Acc: 052.94%, Val. Loss: 1.132, Val. Acc: 053.85%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 008, Train Loss: 1.088, Train Acc: 055.55%, Val. Loss: 1.079, Val. Acc: 055.72%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 009, Train Loss: 1.059, Train Acc: 056.44%, Val. Loss: 1.051, Val. Acc: 057.27%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 010, Train Loss: 1.040, Train Acc: 056.74%, Val. Loss: 1.047, Val. Acc: 056.69%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 011, Train Loss: 1.027, Train Acc: 057.25%, Val. Loss: 1.047, Val. Acc: 055.90%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 012, Train Loss: 1.017, Train Acc: 057.74%, Val. Loss: 1.030, Val. Acc: 056.84%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 013, Train Loss: 1.013, Train Acc: 057.62%, Val. Loss: 1.047, Val. Acc: 055.76%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 014, Train Loss: 1.007, Train Acc: 057.93%, Val. Loss: 1.027, Val. Acc: 057.14%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 015, Train Loss: 1.002, Train Acc: 057.95%, Val. Loss: 1.005, Val. Acc: 058.46%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 016, Train Loss: 0.997, Train Acc: 058.20%, Val. Loss: 1.032, Val. Acc: 057.11%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 017, Train Loss: 0.994, Train Acc: 058.27%, Val. Loss: 0.998, Val. Acc: 058.02%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 018, Train Loss: 0.991, Train Acc: 058.40%, Val. Loss: 1.019, Val. Acc: 057.89%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 019, Train Loss: 0.988, Train Acc: 058.37%, Val. Loss: 1.005, Val. Acc: 058.68%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 020, Train Loss: 0.985, Train Acc: 058.65%, Val. Loss: 1.023, Val. Acc: 057.64%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 021, Train Loss: 0.980, Train Acc: 058.80%, Val. Loss: 1.000, Val. Acc: 058.18%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 022, Train Loss: 0.978, Train Acc: 058.70%, Val. Loss: 1.017, Val. Acc: 057.53%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 023, Train Loss: 0.977, Train Acc: 058.90%, Val. Loss: 1.010, Val. Acc: 057.25%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 024, Train Loss: 0.975, Train Acc: 058.81%, Val. Loss: 0.985, Val. Acc: 058.72%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 025, Train Loss: 0.974, Train Acc: 059.02%, Val. Loss: 0.991, Val. Acc: 058.37%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 026, Train Loss: 0.970, Train Acc: 059.04%, Val. Loss: 0.996, Val. Acc: 057.78%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 027, Train Loss: 0.967, Train Acc: 059.04%, Val. Loss: 0.986, Val. Acc: 058.28%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 028, Train Loss: 0.958, Train Acc: 059.31%, Val. Loss: 0.969, Val. Acc: 059.23%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 029, Train Loss: 0.949, Train Acc: 059.85%, Val. Loss: 1.006, Val. Acc: 057.63%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 030, Train Loss: 0.948, Train Acc: 059.80%, Val. Loss: 0.982, Val. Acc: 057.94%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 031, Train Loss: 0.943, Train Acc: 060.04%, Val. Loss: 0.984, Val. Acc: 058.71%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 032, Train Loss: 0.943, Train Acc: 060.00%, Val. Loss: 0.978, Val. Acc: 058.23%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 033, Train Loss: 0.941, Train Acc: 060.17%, Val. Loss: 0.972, Val. Acc: 058.57%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 034, Train Loss: 0.938, Train Acc: 060.27%, Val. Loss: 1.023, Val. Acc: 057.19%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 035, Train Loss: 0.935, Train Acc: 060.40%, Val. Loss: 0.989, Val. Acc: 058.14%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 036, Train Loss: 0.936, Train Acc: 060.15%, Val. Loss: 0.964, Val. Acc: 059.02%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 037, Train Loss: 0.934, Train Acc: 060.42%, Val. Loss: 0.973, Val. Acc: 058.35%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 038, Train Loss: 0.932, Train Acc: 060.37%, Val. Loss: 0.982, Val. Acc: 058.16%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 039, Train Loss: 0.929, Train Acc: 060.36%, Val. Loss: 0.965, Val. Acc: 059.78%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 040, Train Loss: 0.928, Train Acc: 060.63%, Val. Loss: 0.990, Val. Acc: 057.82%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 041, Train Loss: 0.927, Train Acc: 060.59%, Val. Loss: 0.967, Val. Acc: 059.05%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 042, Train Loss: 0.927, Train Acc: 060.68%, Val. Loss: 0.995, Val. Acc: 057.87%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 043, Train Loss: 0.925, Train Acc: 060.74%, Val. Loss: 0.981, Val. Acc: 058.23%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 044, Train Loss: 0.923, Train Acc: 060.90%, Val. Loss: 0.988, Val. Acc: 058.00%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 045, Train Loss: 0.921, Train Acc: 060.98%, Val. Loss: 0.965, Val. Acc: 058.62%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 046, Train Loss: 0.923, Train Acc: 060.74%, Val. Loss: 0.973, Val. Acc: 058.69%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 047, Train Loss: 0.919, Train Acc: 060.96%, Val. Loss: 0.970, Val. Acc: 058.99%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 048, Train Loss: 0.921, Train Acc: 060.84%, Val. Loss: 0.966, Val. Acc: 059.15%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 049, Train Loss: 0.920, Train Acc: 060.80%, Val. Loss: 0.957, Val. Acc: 059.63%, LR: 1.000000e-03, Linear: False\n",
      "Epoch: 050, Train Loss: 0.918, Train Acc: 061.02%, Val. Loss: 0.987, Val. Acc: 057.79%, LR: 1.000000e-03, Linear: False\n"
     ]
    }
   ],
   "source": [
    "prev_valid_loss = float('inf')\n",
    "\n",
    "linear = LS\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, linear)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, optimizer, criterion, linear)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:03}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:06.2f}%, ' \\\n",
    "          f'Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:06.2f}%, LR: {optimizer.param_groups[0][\"lr\"]:1e}, ' \\\n",
    "          f'Linear: {linear}')\n",
    "        \n",
    "    #scheduler.step()\n",
    "        \n",
    "    if LS and linear:\n",
    "        if prev_valid_loss < valid_loss:\n",
    "            linear = False\n",
    "        prev_valid_loss = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
